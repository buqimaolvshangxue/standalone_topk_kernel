/*
 * Standalone TopK Softmax Kernel Verification
 *
 * Usage: ./verify_topk <experts> <tokens> <dtype> <topk> [--verify]
 * Example: ./verify_topk 64 4 fp16 4 --verify
 *
 * This program:
 * 1. Loads golden data generated by vLLM's fused_topk
 * 2. Runs the standalone kernel with the same input
 * 3. Compares results to verify kernel extraction is correct
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <cmath>
#include <cstdlib>
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cuda_bf16.h>
#include "../src/topk_softmax.h"

// Golden data structure (binary format)
struct GoldenData {
    int tokens, experts, topk;
    int dtype;  // 0=fp32, 1=fp16, 2=bf16
    std::vector<float> input_flat;        // flattened input data
    std::vector<float> exp_weights_flat;  // flattened expected weights
    std::vector<int> exp_indices_flat;    // flattened expected indices
};

// Load golden data from binary file
// File format: [tokens][experts][topk][dtype][input_data][weights_data][indices_data]
GoldenData load_golden(const std::string& path) {
    std::ifstream f(path, std::ios::binary);
    if (!f.is_open()) {
        std::cerr << "ERROR: Cannot open " << path << std::endl;
        exit(1);
    }

    GoldenData data;
    f.read((char*)&data.tokens, sizeof(int));
    f.read((char*)&data.experts, sizeof(int));
    f.read((char*)&data.topk, sizeof(int));
    f.read((char*)&data.dtype, sizeof(int));

    // Read input data (tokens * experts floats)
    int input_size = data.tokens * data.experts;
    data.input_flat.resize(input_size);
    f.read((char*)data.input_flat.data(), input_size * sizeof(float));

    // Read expected weights (tokens * topk floats)
    int weights_size = data.tokens * data.topk;
    data.exp_weights_flat.resize(weights_size);
    f.read((char*)data.exp_weights_flat.data(), weights_size * sizeof(float));

    // Read expected indices (tokens * topk ints)
    data.exp_indices_flat.resize(weights_size);
    f.read((char*)data.exp_indices_flat.data(), weights_size * sizeof(int));

    f.close();
    return data;
}

// Compare results: vLLM vs standalone kernel
bool verify_result(const float* weights_d, const int* indices_d,
                   const GoldenData& g, bool verbose = false) {
    int total = g.tokens * g.topk;
    std::vector<float> weights_host(total);
    std::vector<int> indices_host(total);

    cudaMemcpy(weights_host.data(), weights_d, total * sizeof(float),
               cudaMemcpyDeviceToHost);
    cudaMemcpy(indices_host.data(), indices_d, total * sizeof(int),
               cudaMemcpyDeviceToHost);
    cudaDeviceSynchronize();

    bool all_pass = true;
    int weight_fails = 0;
    int index_fails = 0;

    // Compare each element
    for (int i = 0; i < total; ++i) {
        float weight_diff = std::abs(weights_host[i] - g.exp_weights_flat[i]);
        if (weight_diff > 1e-3) {
            if (verbose && weight_fails < 10) {
                printf("[FAIL] weights[%d]: got %.6f, expected %.6f (diff=%.6f)\n",
                       i, weights_host[i], g.exp_weights_flat[i], weight_diff);
            }
            weight_fails++;
            all_pass = false;
        }

        if (indices_host[i] != g.exp_indices_flat[i]) {
            if (verbose && index_fails < 10) {
                printf("[FAIL] indices[%d]: got %d, expected %d\n",
                       i, indices_host[i], g.exp_indices_flat[i]);
            }
            index_fails++;
            all_pass = false;
        }
    }

    if (!all_pass) {
        printf("Total failures: weights=%d, indices=%d (out of %d)\n",
               weight_fails, index_fails, total);
    }

    return all_pass;
}

void print_usage(const char* prog) {
    printf("Usage: %s <experts> <tokens> <dtype> <topk> [--verify]\n", prog);
    printf("  experts: number of experts (e.g., 64, 128, 256)\n");
    printf("  tokens:  number of tokens (e.g., 4, 16)\n");
    printf("  dtype:   data type (fp32, fp16, bf16)\n");
    printf("  topk:    top-k value (e.g., 2, 4, 6, 8)\n");
    printf("  --verify: verify against golden data\n");
    printf("\nExample: %s 64 4 fp16 4 --verify\n", prog);
}

int main(int argc, char** argv) {
    if (argc < 5) {
        print_usage(argv[0]);
        return 1;
    }

    int experts = atoi(argv[1]);
    int tokens = atoi(argv[2]);
    std::string dtype = argv[3];
    int topk = atoi(argv[4]);
    
    bool verify = (argc > 5 && std::string(argv[5]) == "--verify");

    printf("Configuration: experts=%d, tokens=%d, dtype=%s, topk=%d, verify=%s\n",
           experts, tokens, dtype.c_str(), topk, verify ? "true" : "false");

    // dtype string to code
    int dtype_code = 0;  // 0=fp32, 1=fp16, 2=bf16
    if (dtype == "fp16") dtype_code = 1;
    else if (dtype == "bf16") dtype_code = 2;

    // Construct golden data path
    std::string bin_file = "golden/tokens_" + std::to_string(tokens) +
                           "_experts_" + std::to_string(experts) + "_" +
                           dtype + "_topk" + std::to_string(topk) + ".bin";

    printf("Loading golden data from: %s\n", bin_file.c_str());
    GoldenData golden = load_golden(bin_file);

    // Verify config matches
    if (golden.tokens != tokens || golden.experts != experts ||
        golden.topk != topk || golden.dtype != dtype_code) {
        printf("[ERROR] Golden data config mismatch!\n");
        printf("  Expected: tokens=%d, experts=%d, topk=%d, dtype=%d\n",
               tokens, experts, topk, dtype_code);
        printf("  Got:      tokens=%d, experts=%d, topk=%d, dtype=%d\n",
               golden.tokens, golden.experts, golden.topk, golden.dtype);
        return 1;
    }
    printf("Golden data loaded: tokens=%d, experts=%d, topk=%d\n",
           golden.tokens, golden.experts, golden.topk);

    // Allocate GPU memory
    void* gating_d = nullptr;
    float* weights_d = nullptr;
    int* indices_d = nullptr;

    size_t input_bytes = tokens * experts * (dtype == "fp16" ? 2 : (dtype == "bf16" ? 2 : 4));
    cudaMalloc(&gating_d, input_bytes);
    cudaMalloc(&weights_d, tokens * topk * sizeof(float));
    cudaMalloc(&indices_d, tokens * topk * sizeof(int));

    // Copy input (convert based on dtype)
    if (dtype == "fp16") {
        std::vector<__half> input_flat;
        input_flat.reserve(golden.input_flat.size());
        for (float v : golden.input_flat)
            input_flat.push_back(__float2half(v));
        cudaMemcpy(gating_d, input_flat.data(), input_flat.size() * sizeof(__half),
                   cudaMemcpyHostToDevice);
    } else if (dtype == "bf16") {
        std::vector<__nv_bfloat16> input_flat;
        input_flat.reserve(golden.input_flat.size());
        for (float v : golden.input_flat)
            input_flat.push_back(__float2bfloat16(v));
        cudaMemcpy(gating_d, input_flat.data(), input_flat.size() * sizeof(__nv_bfloat16),
                   cudaMemcpyHostToDevice);
    } else {
        cudaMemcpy(gating_d, golden.input_flat.data(), golden.input_flat.size() * sizeof(float),
                   cudaMemcpyHostToDevice);
    }

    printf("Running standalone kernel (renormalize=false)...\n");

    // Map dtype string to enum
    TopkSoftmaxDtype dtype_enum;
    if (dtype == "fp16") {
        dtype_enum = TopkSoftmaxDtype::Float16;
    } else if (dtype == "bf16") {
        dtype_enum = TopkSoftmaxDtype::BFloat16;
    } else {
        dtype_enum = TopkSoftmaxDtype::Float32;
    }

    // Call topk_softmax - same interface structure as vLLM
    topk_softmax(
        weights_d,              // output: topk_weights
        indices_d,              // output: topk_indices
        nullptr,                // token_expert_indices (allocated internally)
        gating_d,               // input: gating_output
        nullptr,                // softmax_workspace (allocated if needed)
        tokens,                 // num_tokens
        experts,                // num_experts
        topk,                   // topk
        dtype_enum,             // dtype
        false,                  // renormalize (same as golden data)
        0                       // stream (default)
    );
    cudaDeviceSynchronize();

    // Check for CUDA errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("[ERROR] CUDA error: %s\n", cudaGetErrorString(err));
        return 1;
    }

    // Verify: compare standalone kernel results with vLLM golden data
    if (verify) {
        bool pass = verify_result(weights_d, indices_d, golden, true);
        printf("\n========================================\n");
        printf("Result: %s\n", pass ? "[PASS] Kernel extraction correct!" : "[FAIL] Kernel extraction has issues!");
        printf("========================================\n");
        
        // Cleanup
        cudaFree(gating_d);
        cudaFree(weights_d);
        cudaFree(indices_d);
        
        return pass ? 0 : 1;
    }

    printf("Kernel executed successfully\n");

    // Cleanup
    cudaFree(gating_d);
    cudaFree(weights_d);
    cudaFree(indices_d);

    return 0;
}
